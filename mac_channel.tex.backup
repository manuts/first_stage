 \section{Belief Propagation Decoding for 2 User Gaussign MAC}
 The mathematical model of 2 user Gaussign Multiple Access channel is given in Figure~\ref{fig:awgn_channel}. The capacity region is given by
 \begin{eqnarray}
  R^{[1]} &\leq& I(X_1;Y|X_2) \nonumber \\
  R^{[2]} &\leq& I(X_2;Y|X_1) \nonumber \\
  R^{[1]} + R^{[2]} &\leq& I(X_1, X_2;Y) \nonumber
 \end{eqnarray}
The capcity region looks like the curve shown in Figure~\ref{fig:awgn_channel_capacity}. We are intereseted in the rate pairs on the 
dominant face $\mathcal{D}$ of the capacity region for the points on $\mathcal{D}$ gives the maximum sum rate. The corner points are
achievable by successive decoding. The other pointes are achieved through rate splitting or time sharing. Here we present joing decoding
of the two users using belief propogation to achieve a general point on $\mathcal{D}$ without rate splitting or time sharing.
To decode $x_i^{[i]}$, the $i^{th}$ bit of user 1, we have the following MAP decoding rule.
 \begin{eqnarray}
  \hat{x}_i^{[1]} 	&\overset{\Delta}{=}& \text{arg} \max_{x_i}p_{X_i^{[1]}|Y}(x_i^{[1]}|y) \label{eq:mac1} \\
 			&=& \text{arg} \max_{x_i}\sum_{\sim x_i^{[1]}} p_{X^{[1]}, X^{[2]}|Y} (x^{[1]}, x^{[2]}|y)  \label{eq:mac2}\\
 			&=& \text{arg} \max_{x_i}\sum_{\sim x_i^{[1]}} p_{Y|X^{[1]}, X^{[2]}} (y|x^{[1]}, x^{[2]}) p_{X^{[1]}, X^{[2]}}(x^{[1]}, x^{[2]}) \label{eq:mac3}\\
 			&=& \text{arg} \max_{x_i}\sum_{\sim x_i^{[1]}} p_{Y|X^{[1]}, X^{[2]}} (y|x^{[1]}, x^{[2]}) p_{X^{[1]}}(x^{[1]}) p_{X^{[2]}}(x^{[2]}) \label{eq:mac4}\\
 			&=& \text{arg} \max_{x_i}\sum_{\sim x_i^{[1]}} \prod_j p_{Y_j|X_j^{[1]}, X_j^{[2]}} (y_j|x_j^{[1]}, x_j^{[2]}) \mathbf{1}_{\lbrace x^{[1]} \in \mathcal{C}^{[1]}\rbrace} \mathbf{1}_{\lbrace x^{[2]} \in \mathcal{C}^{[2]}\rbrace} \label{eq:mac5}
 \end{eqnarray}
To obtain Equation~\ref{eq:mac2} from Equation~\ref{eq:mac1}, we use the marginal probability rule. We get Equation~\ref{eq:mac3} fron Equation~\ref{eq:mac2} we use
the Bayes' rule. By applying the fact that the data from one user is independent of the data from the other user, we get Equation~\ref{eq:mac4}. Finally we use that
fact that the channel is memoryless, and each codeword is equiprobable to obtain Equation~\ref{eq:mac5}. Figure~\ref{fig:2users_factor} represents the factor graph 
for the marginalization shown in Equation~\ref{eq:mac5}.
 \begin{figure}[scale=1, !tp]
 \centering
  \begin{tikzpicture}
  \draw (0, 0) node[circle, thick, draw] (f) {$+$};
  \path [draw, latex] (-3, -1) -- node[near start, below]{$X_i^{[2]}$} (-1, -1) ;
  \path [draw, latex] (-3, 1) -- node[near start, below]{$X_i^{[1]}$} (-1, 1) ;
  \path [draw, -latex] (-1, -1) -- (f) ;
  \path [draw, -latex] (-1, 1) -- (f) ;  
  \path [draw, -latex] (f) -- node[near end, below]{$Y_i$}(2, 0) ;
  \path [draw, -latex] (0, -2) -- (f);
  \draw (0, -2.3) node{$Z_i \sim \mathcal{N}(0, \sigma^2)$};
   \end{tikzpicture}
\caption{AWGN Multiple Access Channel}
\label{fig:awgn_channel}
\end{figure}

\begin{figure}[scale=1, !tp]
 \centering
  \begin{tikzpicture}
  \path [draw, thick, -latex] (0, 0) -- node[at end, left]{$R^{[2]}$} (0, 5) ;
  \path [draw, thick, -latex] (0, 0) -- node[at end, above]{$R^{[1]}$} (5, 0) ;
  \path [draw, latex] (0, 4) -- (2, 4) ;
  \path [draw, latex] (4, 0) -- (4, 2) ;
  \path [draw, latex] (2, 4) -- (4, 2);
  \draw [dotted, thick] (2, 4) -- (2, 0);
  \draw [dotted, thick] (4, 2) -- (0, 2);
  \draw (-0.75, 2) node{$I(X_2;Y)$};
  \draw (2, -0.25) node{$I(X_1;Y)$};
  \draw (-1, 4) node{$I(X_2;Y|X_1)$};
  \draw (4, -0.25) node{$I(X_1;Y|X_2)$};
  \end{tikzpicture}
\caption{Capacity of AWGN Multiple Access Channel}
\label{fig:awgn_channel_capacity}
\end{figure}
\input{2user_factor}

The message passing rules at the variable nodes and the check nodes remain the same as given for AWGN channel. \cite{2user} suggests the following rule
for the message passing at the function nodes. 
\begin{eqnarray}
 \mu_{s_i \rightarrow x_i^{[2]}} &=& \log \left( \dfrac{\exp( \mu_{x_i^{[1]} \rightarrow s_i}) p(y|x_i^{[1]} = -1, x_i^{[2]} = -1) + p(y|x_i^{[1]} = 1, x_i^{[2]} = -1)}{\exp( \mu_{x_i^{[1]} \rightarrow s_i}) p(y|x_i^{[1]} = -1, x_i^{[2]} = 1) + p(y|x_i^{[1]} = 1, x_i^{[2]} = 1)} \right) \\
 \mu_{s_i \rightarrow x_i^{[1]}} &=& \log \left( \dfrac{\exp( \mu_{x_i^{[2]} \rightarrow s_i}) p(y|x_i^{[1]} = -1, x_i^{[2]} = -1) + p(y|x_i^{[1]} = -1, x_i^{[2]} = 1)}{\exp( \mu_{x_i^{[2]} \rightarrow s_i}) p(y|x_i^{[1]} = 1, x_i^{[2]} = -1) + p(y|x_i^{[1]} = 1, x_i^{[2]} = 1)} \right)
\end{eqnarray}
The above rules have been verified in simulation. It is not clear at first hand why this rule works. Here we present a possible explanation.

From the single user case, the the message incoming into the variable nodes from the channel is of the type, $\log \left( {p(y_i|x_i^{[k]} = -1)}/{p(y_i|x_i^{[k]} = 1)} \right)$.
At the channel output we have the posterior probabilities $p(y_i|x_i^{[1]}, x_i^{[2]})$. Given below is the computation of likelihood for user 2 that is passed to the variable node.
\begin{eqnarray}
  \frac{p(y_i|x_i^{[2]} = -1)}{p(y_i|x_i^{[2]} = 1)} &=& \frac{p(y_i|x_i^{[1]}=-1, x_i^{[2]}=-1)p(x_i^{[1]}=-1|x_i^{[2]}) + p(y_i|x_i^{[1]}=1, x_i^{[2]}=-1)p(x_i^{[1]}=1|x_i^{[2]})}{p(y_i|x_i^{[1]}=-1, x_i^{[2]}=1)p(x_i^{[1]}=-1|x_i^{[2]}) + p(y_i|x_i^{[1]}=1, x_i^{[2]}=1)p(x_i^{[1]}=1|x_i^{[2]})}
%&=& \frac{p(y_i|x_i^{[1]}=-1, x_i^{[2]}=-1)\dfrac{p(x_i^{[1]}=-1|x_i^{[2]})}{p(x_i^{[1]}=1|x_i^{[2]})} + p(y_i|x_i^{[1]}=1, x_i^{[2]}=-1)}{p(y_i|x_i^{[1]}=-1, x_i^{[2]}=1)\dfrac{p(x_i^{[1]}=-1|x_i^{[2]})}{p(x_i^{[1]}=1|x_i^{[2]})} + p(y_i|x_i^{[1]}=1, x_i^{[2]}=1)} \nonumber \\
\end{eqnarray}

This should be read as a ratio of marginal probabilities where $p(x_i^{[1]}|x_i^{[2]})$ acts as the weights of the conditional probabilities. To start with $x_i^{[1]}$ and $x_i^{[2]}$ 
are independent. So we can use $p(x_i^{[1]})$ instead of $p(x_i^{[1]}|x_i^{[2]})$. After running an iteration of message passing on one user, the posterior probability $p(x_i^{[1]}|y)$
changes and this should be used as the weights in the above equation. So we should update the above eqation as 
\begin{equation}
 \frac{p(y_i|x_i^{[2]} = -1)}{p(y_i|x_i^{[2]} = 1)} = \frac{p(y_i|x_i^{[1]}=-1, x_i^{[2]}=-1)p(x_i^{[1]}=-1|y}) + p(y_i|x_i^{[1]}=1, x_i^{[2]}=-1)p(x_i^{[1]}=1|y})}{p(y_i|x_i^{[1]}=-1, x_i^{[2]}=1)p(x_i^{[1]}=-1|x_i^{[2]}) + p(y_i|x_i^{[1]}=1, x_i^{[2]}=1)p(x_i^{[1]}=1|x_i^{[2]})} 
\end{equation}